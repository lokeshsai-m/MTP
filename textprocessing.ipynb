{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrWsJBE0UqhjFzI15cLntx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokeshsai-m/MTP/blob/main/textprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Processing"
      ],
      "metadata": {
        "id": "bVtjQrxbo-Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTOwoE1W-Imb",
        "outputId": "63cb159a-5149-41f5-d6cf-35e73116f6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        "\n",
        "def textprocessing_sentence(sentence):\n",
        "    text=sentence.lower()\n",
        "    # text = re.sub(r'\\d+', '', text)\n",
        "    temp_str = text.split()\n",
        "    new_string = []\n",
        "    for word in temp_str:\n",
        "      if word.isdigit():\n",
        "        temp = p.number_to_words(word)\n",
        "        new_string.append(temp)\n",
        "\n",
        "      else:\n",
        "        new_string.append(word)\n",
        "\n",
        "    temp_str = ' '.join(new_string)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text=temp_str.translate(translator)\n",
        "\n",
        "    text=\" \".join(text.split())\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    filtered_str = ' '.join(filtered_text)\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    word_tokens = word_tokenize(filtered_str)\n",
        "\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Stem each word in the sentence\n",
        "    stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
        "\n",
        "    # Convert the list of stemmed words into a single string\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "\n",
        "    return stemmed_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "FeTDyfYv5SSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text vectorization using TF-IDF"
      ],
      "metadata": {
        "id": "eXVIyDYVo3CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "index = 0\n",
        "while index < len(documents):\n",
        "    documents[index]=textprocessing_sentence(documents[index])\n",
        "    print(documents[index])\n",
        "    index += 1\n",
        "\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "import pandas as pd\n",
        "df_tfidf = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(df_tfidf)\n",
        "\n",
        "document_idx = 0\n",
        "tfidf_values_for_doc = dict(zip(feature_names, tfidf_matrix.toarray()[document_idx]))\n",
        "print(\"\\nTF-IDF values for Document {}: {}\".format(document_idx + 1, tfidf_values_for_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8paN7S2dTHr2",
        "outputId": "906e05cf-d8a1-4759-f0dd-b35ac4867c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first document\n",
            "document second document\n",
            "third one\n",
            "first document\n",
            "TF-IDF Matrix:\n",
            "   document     first       one    second     third\n",
            "0  0.629228  0.777221  0.000000  0.000000  0.000000\n",
            "1  0.787223  0.000000  0.000000  0.616668  0.000000\n",
            "2  0.000000  0.000000  0.707107  0.000000  0.707107\n",
            "3  0.629228  0.777221  0.000000  0.000000  0.000000\n",
            "\n",
            "TF-IDF values for Document 1: {'document': 0.6292275146695526, 'first': 0.7772211620785797, 'one': 0.0, 'second': 0.0, 'third': 0.0}\n"
          ]
        }
      ]
    }
  ]
}